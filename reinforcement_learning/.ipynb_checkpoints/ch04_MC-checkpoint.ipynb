{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Methods 蒙特卡洛方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Prediction\n",
    "\n",
    "- Model-Based 但无需状态转移概率\n",
    "\n",
    "\n",
    "- 遵循 policy $\\pi$ 通过 Model 生成 state, action, reward 的序列.\n",
    "\n",
    "\n",
    "\n",
    "- **核心思想**:使用完整 returns 的算术平均值来逼近 returns 的期望值.因此更新发生在片段的结束.\n",
    "\n",
    "\n",
    "- 通常一个 state s 在片段中可能出现多次,如果只在 s 第一次出现时统计 returns 的算数平均值,这种方法称为 *first-visit* MC.(之后学习 *every-visit* MC)\n",
    "\n",
    "\n",
    "- 算法伪码(使用GPI):\n",
    "\n",
    "\n",
    "![first-visit MC](../images/first_visit_MC.png)\n",
    "\n",
    "\n",
    "- 相对于 DP 的优点:\n",
    "\n",
    "    1. MC 不是 **expected update** 而是 **sample update** . 因此无需状态转移概率.\n",
    "\n",
    "    2. MC 对每个状态的估计是独立的. 不需要 **bootstrap** (如贝尔曼方程某个状态需要其后继状态估计)\n",
    "\n",
    "    3. 可以从感兴趣的状态开始学习."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MC Control\n",
    "\n",
    "\n",
    "- 当了解 model 时,可以单独通过价值函数确定 policy.正如 DP 的 Value Iteration.\n",
    "\n",
    "\n",
    "- 不了解 model 时,通过估计行为价值函数来确定 policy. MC 对行为价值函数的估计类似与价值函数也使用算术平均值逼近.\n",
    "\n",
    "\n",
    "- 因为初始化 policy $\\pi$ 是固定的,所以行为价值函数的 action 也是固定的,这样不利于 Exploration. 解决方法有两种: *exploring starts* 和 *$\\epsilon -soft$* .\n",
    "\n",
    "\n",
    "- exploring starts: 初始化 state-action 对时随机选取 action. 随着片段数目的增加, state-action 对的访问数也增加,算法伪码:\n",
    "\n",
    "![MCES](../images/MC_ES.png)\n",
    "\n",
    "\n",
    "- $\\epsilon -soft$: $\\pi(a|s) > 0, \\forall s \\in \\mathcal{S}, \\forall a \\in \\mathcal{A(s)}$ ,任一 state s 的相关 action 都有可能执行,但最终策略将固定. \n",
    "\n",
    "\n",
    "- $\\epsilon -greedy$ 是 $\\epsilon -soft$ 的一种实现方式, 算法伪码:\n",
    "\n",
    "![MCSOFT](../images/MC_SOFT.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On-policy 和 Off-policy\n",
    "\n",
    "\n",
    "- on-policy 估计和更新生成数据的策略\n",
    "\n",
    "\n",
    "- off-policy 估计和更新的策略不生成数据,通常估计 target policy,但是数据遵循 behavior policy 生成.\n",
    "\n",
    "\n",
    "- importance-sampling (IS,重要性采样),可估计不同分布的数学期望.\n",
    "\n",
    "\n",
    "$$\\mathbb{E}_{x \\sim P}[f(x)] =\\sum P(x)f(x)=\\sum \\frac{P(x)}{Q(x)}Q(x)f(x) = \\mathbb{E}_{x \\sim Q}\\left[ \\frac{P(x)}{Q(x)} f(x) \\right]$$\n",
    "\n",
    "\n",
    "\n",
    "- importance-sampling ratio (重要性采样比率),假设 $\\pi$ 是 target policy, b 是 behavior policy.\n",
    "\n",
    "\n",
    "$$\\rho_{t:T-1} \\dot= \\frac{\\prod_{k=t}^{T-1}\\pi(A_k|S_k)p(S_{k+1}|S_k,A_k)}{\\prod_{k=t}^{T-1}b(A_k|S_k)p(S_{k+1}|S_k,A_k)} = \\prod_{k=t}^{T-1}\\frac{\\pi(A_k|S_k)}{b(A_k|S_k)}$$\n",
    "\n",
    "\n",
    "因此,如果:\n",
    "\n",
    "$$v_b(s) = \\mathbb{E}_{G \\sim b}[G_t|S_t=s]$$\n",
    "\n",
    "那么:\n",
    "\n",
    "$$v_\\pi(s) = \\mathbb{E}_{G \\sim b}[\\rho_{t:T-1}G_t|S_t=s]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinary IS and Weighted IS\n",
    "\n",
    "\n",
    "- OIS\n",
    "\n",
    "\n",
    "$$V(s) \\dot= \\frac{\\sum_{t \\in \\mathcal{J(s)}}\\rho_{t:T(t)-1}G_t}{\\left|\\mathcal{J(s)}\\right|}$$\n",
    "\n",
    "\n",
    "- WIS\n",
    "\n",
    "\n",
    "$$V(s) \\dot= \\frac{\\sum_{t \\in \\mathcal{J(s)}}\\rho_{t:T(t)-1}G_t}{\\sum_{t \\in \\mathcal{J(s)}}\\rho_{t:T(t)-1}}$$\n",
    "\n",
    "\n",
    "其中: $\\mathcal{J(s)}$ 表示在 state s 上的时间步集合.\n",
    "\n",
    "\n",
    "-  OIS 无偏差,方差大; WIS 有偏估计,方差小."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremental Implementation\n",
    "\n",
    "\n",
    "- WIS 的更新规则:\n",
    "\n",
    "\n",
    "$$V_{n+1} \\dot= V_n + \\frac{W_n}{C_n}\\left[G_n - V_n\\right], n \\ge 1$$\n",
    "\n",
    "\n",
    "其中:  $C_{n+1} \\dot= C_n + W_{n+1}, C_0 \\dot= 0$\n",
    "\n",
    "\n",
    "- Off-policy MC prediction for estimating $Q \\to q_\\pi$:\n",
    "\n",
    "\n",
    "![policy evaluation](../images/off-policy_evaluation.png)\n",
    "\n",
    "\n",
    "- Off-policy MC control, for estimating $\\pi \\to \\pi_*$\n",
    "\n",
    "\n",
    "![control](../images/off-policy_control.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "230px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
