{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Methods 蒙特卡洛方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Prediction\n",
    "\n",
    "- Model-Based 但无需状态转移概率\n",
    "\n",
    "\n",
    "- 通过 Model 生成 state, action, reward 的序列.\n",
    "\n",
    "\n",
    "\n",
    "- **核心思想**:使用 returns 的算术平均值来逼近期望值.\n",
    "\n",
    "\n",
    "- 通常一个 state s 在片段中可能出现多次,如果只在 s 第一次出现时统计 returns 的算数平均值,这种方法称为 *first-visit* MC.(之后学习 *every-visit* MC)\n",
    "\n",
    "\n",
    "- 算法伪码:\n",
    "\n",
    "\n",
    "![first-visit MC](../images/first_visit_MC.png)\n",
    "\n",
    "\n",
    "\n",
    "- MC 不是 **expected update** 而是 **sample update** .\n",
    "\n",
    "\n",
    "- MC 对每个状态的估计是独立的. 不需要 **bootstrap** (如贝尔曼方程某个状态需要其后继状态估计)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MC Control\n",
    "\n",
    "\n",
    "- 当了解 model 时,可以单独通过价值函数确定 policy.正如 DP 的 Value Iteration.\n",
    "\n",
    "\n",
    "- 不了解 model 时,通过估计行为价值函数来确定 policy.\n",
    "\n",
    "\n",
    "- 因为初始化 policy $\\pi$ 是固定的,所以行为价值函数的 action 也是固定的,这样不利于 Exploration. 解决方法有两种: *exploring starts* 和 *$\\epsilon -soft$* .\n",
    "\n",
    "\n",
    "- exploring starts: 初始化 state-action 对时随机选取 action. 算法伪码:\n",
    "\n",
    "![MCES](../images/MC_ES.png)\n",
    "\n",
    "\n",
    "- $\\epsilon -soft$: $\\pi(a|s) > 0, \\forall s \\in \\mathcal{S}, \\forall a \\in \\mathcal{A(s)}$ ,任一 state s 的相关 action 都有可能执行,但最终策略将固定. 算法伪码:\n",
    "\n",
    "![MCSOFT](../images/MC_SOFT.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
