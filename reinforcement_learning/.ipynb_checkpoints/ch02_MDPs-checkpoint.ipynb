{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Processes (MDPs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Returns 收益\n",
    "\n",
    "returns 是时间步 t 之后所有 rewards 的函数.对所有 rewards 进行求和是最简单的函数形式,但是对于持续任务函数值会趋于无穷,因此通常会加入折扣系数 $\\gamma$ .假设有限任务在终止状态时之后的所有 rewards 为 0,可以将 returns 统一定义为:\n",
    "$$G_t \\dot= \\sum_{k=t+1}^{T}\\gamma^{k-t-1}R_k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Property\n",
    "\n",
    "- History 历史\n",
    "\n",
    "History 是 state, action, reward 的序列. 如: $H_t = S_0, A_0, R_1, S_1, A_1, \\cdots , S_{t-1}, A_{t-1}, R_t, S_t$\n",
    "\n",
    "- Markov Property 马尔科夫性\n",
    "\n",
    "如果 $S_t$ 是 Markov Property 的, 则以后的 History 只需 $S_t$ 就可以确认,而无须了解 $S_t$ 以前的 History.\n",
    "\n",
    "$$\\mathbb{P}[S_{t+1}, R_{t+1} | S_t, A_t] = \\mathbb{P}[S_{t+1}, R_{t+1} | History]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPDs 相关概念\n",
    "\n",
    "- 时间步 t-1 时刻 $S_{t-1} = s$, $A_{t-1} = a$,那么随机变量 $s' \\in \\mathcal{S}$, $r \\in \\mathcal{R}$ 在时间步 t 时刻发生的条件概率:\n",
    "\n",
    "$$p(s',r|s, a) = \\mathbb{P}[S_t = s', R_t= r | S_{t-1} = s, A_{t-1} = a]$$\n",
    "\n",
    "其中:\n",
    "\n",
    "$$\\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s',r|s,a) = 1$$\n",
    "\n",
    "- 状态转移概率, $p: \\mathcal{S} \\times \\mathcal{S} \\times \\mathcal{A} \\to [0,1]$:\n",
    "\n",
    "$$p(s'|s,a)  \\dot=  \\mathbb{P}[S_t = s'|S_{t-1} = s, A_{t-1} = a] = \\sum_{r \\in \\mathcal{R}}p(s', r|s, a)$$\n",
    "\n",
    "- 给定 state, action 所期望 t 时刻的 reward, $r: \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}$:\n",
    "\n",
    "$$r(s,a) \\dot= \\mathbb{E}[R_t|S_{t-1} = s, A_{t-1} = a] = \\sum_{r \\in \\mathcal{R}}  r \\sum_{s' \\in \\mathcal{S}} p(s',r|s,a)$$\n",
    "\n",
    "- 给定 state, action 和 后继 state 所期望的 reward, $r: \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\to \\mathbb{R}$\n",
    "\n",
    "$$r(s, a, s') \\dot= \\mathbb{E}[R_t|S_{t-1} = s, A_{t-1} =a , S_t = s']= \\sum_{r \\in \\mathcal{R}} r \\frac{p(s',r|s,a)}{p(s'|s,a)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Equation 贝尔曼方程\n",
    "\n",
    "- Value Function 价值函数\n",
    "\n",
    "在时间步 t 时刻, $\\forall{s} \\in \\mathcal{S}$ 状态下,不同的策略或行为都会得到不同的 returns ,我们需要使用价值函数来评估不同的 returns:\n",
    "$$v(s) \\dot= \\mathbb{E}[G_t|S_t=s]$$\n",
    "\n",
    "- policy 策略\n",
    "\n",
    "policy 是由 state 到 action 采取概率的映射, $\\pi(a|s)$ 表示当 $S_t=s$ 时, $A_t =a$的采取概率.\n",
    "\n",
    "- 当 state s 遵循 policy $\\pi$ 时, 定义价值函数:\n",
    "\n",
    "$$v_{\\pi}(s) \\dot= \\mathbb{E_{\\pi}}[G_t|S_t=s]$$\n",
    "\n",
    "- 当 state s 遵循 policy $\\pi$ 且选择 action a 时, 定义行为价值函数 (action-value function):\n",
    "\n",
    "$$q_{\\pi}(s,a) \\dot= \\mathbb{E_{\\pi}}[G_t|S_t=s, A_t=a]$$\n",
    "\n",
    "- 贝尔曼方程描述了状态值与所有后继状态值的关系:\n",
    "\n",
    "$$v_{\\pi}(s) = \\sum_a \\pi(a|s) \\sum_{s', r} p(s', r|s,a)[r + \\gamma v_{\\pi}(s')]$$\n",
    "\n",
    "- 最优策略\n",
    "\n",
    "policy $\\pi$ 优于 policy $\\pi'$, 当且仅当 $v_{\\pi}(s) >= v_{\\pi'}(s)$, $\\forall{s} \\in \\mathcal{S}$.总是会存在一个或多个 policy 优于其他所有的 policies, 我们称之为最优策略 $\\pi_*$.\n",
    "\n",
    "- 最优价值函数: \n",
    "$$v_*(s) \\dot= \\max_{\\pi}v_{\\pi}(s), \\forall{s} \\in \\mathcal{S}$$\n",
    "\n",
    "- 最优行为价值函数:\n",
    "\n",
    "$$q_*(s) \\dot= \\max_{\\pi} q_{\\pi}(s,a), \\forall{s} \\in \\mathcal{S}, a \\in \\mathcal{A}(s)$$\n",
    "\n",
    "- 贝尔曼最优方程\n",
    "\n",
    "贝尔曼方程直观的表述了,某个状态的最优价值函数值等于在该状态下遵循最优策略采取最大化行为得到的收益期望值. \n",
    "\n",
    "$$v_*(s) = \\max_a \\mathbb{E}_{\\pi_*}[G_t|S_t=s, A_t = a] = \\max_{a \\in \\mathcal{A}(s)} q_{\\pi_*}(s,a) = \\max_a \\sum_{s',r} p(s',r|s,a)[r + \\gamma v_*(s')]$$\n",
    "\n",
    "$$q_*(s,a) = \\mathbb{E}[R_{t+1} + \\gamma v_*(s') | S_t = s, A_t = a] = \\sum_{s',r}p(s',r |s,a)[r + \\gamma\\max_{a'} q_*(s', a')]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
