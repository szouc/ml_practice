{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Programming (DP) 动态规划"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最优解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative policy evaluation\n",
    "\n",
    "- 已知环境动态和 policy $\\pi$情况下, 可以通过贝尔曼方程解得 $v_{\\pi}(s), \\forall s \\in \\mathcal{S}$,但是求解过程复杂. \n",
    "\n",
    "\n",
    "- 根据贝尔曼方程,通过迭代方式不断更新 $v_k(s)$ ,当 $k \\to \\infty$ 时, $v_k(s) = v_{\\pi}(s)$, 这种方式称为 Iterative policy evaluation .\n",
    "\n",
    "\n",
    "- 遵循 policy $\\pi$ 用上一次迭代的 state s 的后继价值函数,来**估计(预测)**当前片段的 state s 的价值函数.\n",
    "\n",
    "\n",
    "$$v_{k+1}(s) \\dot= \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma v_k(S_{t+1}) | S_t = s] = \\sum_{a} \\pi(a|s) \\sum_{s',r} p(s', r|s, a)[r + \\gamma v_k(s')]$$\n",
    "\n",
    "- 算法伪码\n",
    "\n",
    "![Iterative Policy Evaluation](../images/iterative_policy_evaluation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Improvement\n",
    "\n",
    "- 估计 $v_{\\pi}(s)$ 的目的是改进 policy ${\\pi}$ .\n",
    "\n",
    "\n",
    "- 行为价值函数 $q_{\\pi}(s, a)$ 可以通过 $v_{\\pi}(s)$ 求出:\n",
    "\n",
    "\n",
    "$$q_{\\pi}(s,a) \\dot= \\mathbb{E}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1} | S_t= s, A_t = a] = \\sum_{s', r} p(s', r|s, a) \\left[ r + \\gamma v_{\\pi}(s') \\right]$$\n",
    "\n",
    "\n",
    "- **Policy Improvement**: 可以证明 $v_{\\pi}(s') \\ge q_{\\pi}(s, \\pi'(s) \\ge v_{\\pi}(s)$, 因此 policy $\\pi'$ 优于 policy $\\pi$\n",
    "\n",
    "\n",
    "- 在 state s 处, policy $\\pi'$ 可以通过贪心算法给出:\n",
    "\n",
    "\n",
    "$$\\pi'(s) \\dot= \\arg\\max_a q_{\\pi}(s,a) = \\arg\\max_a \\sum_{s',r} p(s', r|s,a) \\left[ r + \\gamma v_{\\pi}(s') \\right]$$\n",
    "\n",
    "\n",
    "- 假设对所有的 state s , policy ${\\pi'}$ 都不在优于 ${\\pi}$, 那么可以得出:\n",
    "\n",
    "\n",
    "$$v_{\\pi}(s) = v_{\\pi'}(s) = v_*(s) = \\max_a \\sum_{s', r} p(s', r|s,a) \\left[ r + \\gamma v_{\\pi'}(s') \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration\n",
    "\n",
    "\n",
    "- 可以通过不断迭代的方式,求得最优策略:\n",
    "\n",
    "\n",
    "$$\\pi_0 \\stackrel{E}\\longrightarrow v_{\\pi_0} \\stackrel{I}\\longrightarrow \\pi_1 \\stackrel{E}\\longrightarrow v_{\\pi_1} \\stackrel{I}\\longrightarrow \\cdots \\pi_* \\stackrel{E}\\longrightarrow v_{\\pi_*} $$\n",
    "\n",
    "\n",
    "- 算法伪码:\n",
    "\n",
    "\n",
    "![policy iteration](../images/policy_iteration.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration\n",
    "\n",
    "\n",
    "- Policy Iteration 建立在贝尔曼方程前提上,先估计出 $v_{\\pi}(s)$,然后更新找到 $v_*(s)$. 缺点是每次 Policy Evaluation 阶段都要收敛到一个预设值,然后再进行 Policy Improvement 阶段. \n",
    "\n",
    "\n",
    "- 另外一种方式是建立在贝尔曼最优方程迭代上,如果 $v_{\\pi}(s) = v_*(s)$,当且仅当所有 state s 的后继状态 $v_{\\pi}(s') = v_*(s')$. 因此可以不断迭代 $v_k(s)$ ,当 $k \\to \\infty$ 时, $v_k(s) = v_*(s)$.\n",
    "\n",
    "\n",
    "- 无须在意 policy, 用上一次迭代的 state s 的后继最优价值函数,来**估计(预测)**当前片段的 state s 的最优价值函数.\n",
    "\n",
    "$$v_{k+1}(s) \\dot= \\max_a \\mathbb{E}[ R_{t+1} + \\gamma v_k(S_{k+1} | S_t=s, A_t=a] = \\max_a \\sum_{s',r}p(s', r|s,a) \\left[ r + \\gamma v_k(s') \\right]$$\n",
    "\n",
    "\n",
    "- 算法伪码:\n",
    "\n",
    "![value_iteration](../images/value_iteration.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
