{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On-policy prediction with approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value-function Approximation （价值函数近似法）\n",
    "\n",
    "\n",
    "- update notation $s \\mapsto u$，其中 $s$ 是更新后的状态，$u$ 是 $s$ 的估计价值函数所要达到的目标:\n",
    "\n",
    "    - **MC**: $S_t \\mapsto G_t$\n",
    "    - **TD(0)**: $S_t \\mapsto R_{t+1} + \\gamma \\hat{v}(S+{t+1}, \\mathbf{w}_t)$\n",
    "    - **n-step TD**: $S_t \\mapsto G_{t:t+n}$\n",
    "    - **DP**: $s \\mapsto \\mathbb{E}\\left[R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\mathbf{w}_t) \\middle| S_t = s\\right]$\n",
    "    \n",
    "\n",
    "- Function Approximation 函数近似法使用 $s \\mapsto g$ 近似 $u$ 。\n",
    "\n",
    "\n",
    "- 表格式每次更新当前状态，其他状态不更新；近似法会对当前状态和其他部分或全部状态同时更新。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Prediction Objective($\\overline{VE}$) 预测目标\n",
    "\n",
    "\n",
    "- 近似法的更新会同时影响多个状态，所以需要确定状态的重要程度。定义状态的一种分布：$\\mu(s) \\ge 0, \\sum_s \\mu(s) = 1$，分布表示状态对损失（error）的重视程度。（通常使用时间步数的分数表示）如果是 on-policy 训练则称这种分布为 on-policy 分布。\n",
    "\n",
    "\n",
    "- 损失函数定义为估计价值函数与真实价值函数差的平方的数学期望（均方误差）：\n",
    "\n",
    "\n",
    "$$\\overline{VE}(\\mathbf{w}) \\dot= \\sum_{s\\in \\mathcal{S}} \\mu(s) \\left[v_{\\pi}(s) - \\hat{v}(s, \\mathbf{w})\\right]^2$$\n",
    "\n",
    "\n",
    "- 片段式任务的 on-policy 分布：\n",
    "\n",
    "\n",
    "$$\\eta(s) = h(s) + \\sum_\\bar{s}\\eta(\\bar{s})\\sum_a \\pi(a|\\bar{s})p(s|\\bar{s},a), \\forall{s} \\in \\mathcal{S}$$\n",
    "\n",
    "其中： $h(s)$ 是 $s$ 为片段的初始状态的概率，$\\eta(s)$ 表示到达 $s$ 的时间步数。\n",
    "\n",
    "可知 on-policy 分布:\n",
    "\n",
    "\n",
    "$$\\mu(s)=\\frac{\\eta(s)}{\\sum_{s'}\\eta(s')}, \\forall{s} \\in \\mathcal{S}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic-gradient and Semi-gradient Methods 随机梯度和半梯度法\n",
    "\n",
    "\n",
    "- 随机梯度下降 （SGD）方法是通过在每个示例之后对权重向量进行少量调整，以最大程度地减少误差\n",
    "\n",
    "\n",
    "$$\\mathbf{w}_{t+1} \\dot= \\mathbf{w}_t - \\frac{1}{2} \\alpha \\nabla \\left[v_{\\pi}(S_t) - \\hat{v}(S_t,\\mathbf{w}_t)\\right]^2 = \\mathbf{w}_t + \\alpha \\left[v_{\\pi}(S_t) - \\hat{v}(S_t, \\mathbf{w}_t)\\right] \\nabla \\hat{v}(S_t, \\mathbf{w}_t)$$\n",
    "\n",
    "\n",
    "其中, $\\alpha$ 是一个正数，导数向量 $\\nabla f(\\mathbf{w})$ 称为 $f$ 关于 $\\mathbf{w}$ 的梯度，是误差下降最快的方向。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
