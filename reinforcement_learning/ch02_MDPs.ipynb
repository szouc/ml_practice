{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Processes (MDPs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Returns 收益\n",
    "\n",
    "- 在非关联问题中,因为每个 state 都是 iid 的,所以只需知道每个 state 的即时最大 reward,就可以获得最大收益.\n",
    "\n",
    "\n",
    "- 而在关联问题中,每个 state 都会影响其后的 state,因此除了关注即时的 reward 之后,还需要考虑长久的累积 reward.\n",
    "\n",
    "\n",
    "- returns 是时间步 t 之后所有 rewards 的函数.对所有 rewards 进行求和是最简单的函数形式,但是对于持续任务函数值会趋于无穷,因此通常会加入折扣系数 $\\gamma$ .假设有限任务在终止状态时之后的所有 rewards 为 0,可以将 returns 统一定义为:\n",
    "\n",
    "\n",
    "$$G_t \\dot= \\sum_{k=t+1}^{T}\\gamma^{k-t-1}R_k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Property\n",
    "\n",
    "- History 历史\n",
    "\n",
    "History 是 state, action, reward 的序列. 如: $H_t = S_0, A_0, R_1, S_1, A_1, \\cdots , S_{t-1}, A_{t-1}, R_t, S_t$\n",
    "\n",
    "- Markov Property 马尔科夫性\n",
    "\n",
    "如果 $S_t$ 是 Markov Property 的, 则以后的 History 只需 $S_t$ 就可以确认,而无须了解 $S_t$ 以前的 History.\n",
    "\n",
    "$$\\mathbb{P}[S_{t+1}, R_{t+1} | S_t, A_t] = \\mathbb{P}[S_{t+1}, R_{t+1} | History]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPDs 相关概念\n",
    "\n",
    "- 时间步 t-1 时刻 $S_{t-1} = s$, $A_{t-1} = a$,那么随机变量 $s' \\in \\mathcal{S}$, $r \\in \\mathcal{R}$ 在时间步 t 时刻发生的联合概率:\n",
    "\n",
    "$$p(s',r|s, a) = \\mathbb{P}[S_t = s', R_t= r | S_{t-1} = s, A_{t-1} = a]$$\n",
    "\n",
    "其中:\n",
    "\n",
    "$$\\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s',r|s,a) = 1$$\n",
    "\n",
    "- 状态转移概率,即 $s'$ 的边缘概率: $p: \\mathcal{S} \\times \\mathcal{S} \\times \\mathcal{A} \\to [0,1]$:\n",
    "\n",
    "$$p(s'|s,a)  \\dot=  \\mathbb{P}[S_t = s'|S_{t-1} = s, A_{t-1} = a] = \\sum_{r \\in \\mathcal{R}}p(s', r|s, a)$$\n",
    "\n",
    "- 给定 state, action 所期望 t 时刻的 reward, $r: \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}$:\n",
    "\n",
    "$$r(s,a) \\dot= \\mathbb{E}[R_t|S_{t-1} = s, A_{t-1} = a] = \\sum_{r \\in \\mathcal{R}} r p(r | s,a)= \\sum_{r \\in \\mathcal{R}}  r \\sum_{s' \\in \\mathcal{S}} p(s',r|s,a)$$\n",
    "\n",
    "- 给定 state, action 和 后继 state 所期望的 reward, $r: \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\to \\mathbb{R}$\n",
    "\n",
    "$$r(s, a, s') \\dot= \\mathbb{E}[R_t|S_{t-1} = s, A_{t-1} =a , S_t = s']= \\sum_{r \\in \\mathcal{R}} r p(r|s, a, s') = \\sum_{r \\in \\mathcal{R}} r \\frac{p(s',r|s,a)}{p(s'|s,a)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Equation 贝尔曼方程\n",
    "\n",
    "- Value Function 价值函数\n",
    "\n",
    "在时间步 t 时刻, $\\forall{s} \\in \\mathcal{S}$ 状态下,不同的策略或行为都会得到不同的 returns ,定义价值函数 $v(s)$ 是 state s 时 returns 的期望值:\n",
    "$$v(s) \\dot= \\mathbb{E}[G_t|S_t=s]$$\n",
    "\n",
    "- policy 策略\n",
    "\n",
    "policy 是由 state 到 action 采取概率的映射, $\\pi(a|s)$ 表示当 $S_t=s$ 时, $A_t =a$的采取概率.\n",
    "\n",
    "- 当 state s 遵循 policy $\\pi$ 时, 定义价值函数:\n",
    "\n",
    "$$v_{\\pi}(s) \\dot= \\mathbb{E_{\\pi}}[G_t|S_t=s] = \\mathbb{E_{\\pi}} \\left[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\Bigg| S_t =s \\right], \\forall s \\in \\mathcal{S}$$\n",
    "\n",
    "- 当 state s 遵循 policy $\\pi$ 且选择 action a 时, 定义行为价值函数 (action-value function):\n",
    "\n",
    "$$q_{\\pi}(s,a) \\dot= \\mathbb{E_{\\pi}}[G_t|S_t=s, A_t=a] = \\mathbb{E_{\\pi}} \\left[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\Bigg| S_t =s, A_t=a \\right], \\forall s \\in \\mathcal{S}$$\n",
    "\n",
    "- 贝尔曼方程描述了当前状态的价值函数与所有后继状态的价值函数的关系:\n",
    "\n",
    "$$v_{\\pi}(s) = \\mathbb{E}_{\\pi}[R_{t+1}+ \\gamma G_{t+1}|S_t=s] = \\mathbb{E}_{\\pi}[R_{t+1}+ \\gamma v_{\\pi}(s')|S_t=s] = \\sum_a \\pi(a|s) \\sum_{s', r} p(s', r|s,a)[r + \\gamma v_{\\pi}(s')]$$\n",
    "\n",
    "- 最优策略\n",
    "\n",
    "policy $\\pi$ 优于 policy $\\pi'$, 当且仅当 $v_{\\pi}(s) >= v_{\\pi'}(s)$, $\\forall{s} \\in \\mathcal{S}$.总是会存在一个或多个 policy 优于其他所有的 policies, 我们称之为最优策略 $\\pi_*$.\n",
    "\n",
    "- 最优价值函数: \n",
    "$$v_*(s) \\dot= \\max_{\\pi}v_{\\pi}(s), \\forall{s} \\in \\mathcal{S}$$\n",
    "\n",
    "- 最优行为价值函数:\n",
    "\n",
    "$$q_*(s) \\dot= \\max_{\\pi} q_{\\pi}(s,a), \\forall{s} \\in \\mathcal{S}, a \\in \\mathcal{A}(s)$$\n",
    "\n",
    "- 一个确定的最优策略:\n",
    "\n",
    "$$\\pi_*(a|s) = \\begin{cases} 1,if a = \\arg\\max_{a \\in \\mathcal{A}} q_*(s,a)\\\\ 0, otherwise\\end{cases}$$\n",
    "\n",
    "- 贝尔曼最优方程\n",
    "\n",
    "上述最优策略直观的表述了,某个状态的最优价值函数等于在该状态下采取最大化行为的最优行为价值函数. \n",
    "\n",
    "$$v_*(s) = \\max_a \\mathbb{E}_{\\pi_*}[G_t|S_t=s, A_t = a] = \\max_{a \\in \\mathcal{A}(s)} q_{\\pi_*}(s,a) = \\max_a \\sum_{s',r} p(s',r|s,a)[r + \\gamma v_*(s')]$$\n",
    "\n",
    "$$q_*(s,a) = \\mathbb{E}[R_{t+1} + \\gamma v_*(s') | S_t = s, A_t = a] = \\sum_{s',r}p(s',r |s,a)[r + \\gamma\\max_{a'} q_*(s', a')]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
